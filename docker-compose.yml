version: '3.8'

services:
  # ollama service removed to use Local Host Ollama (GPU accel)

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: civ_webui
    restart: unless-stopped
    ports:
      - "3000:8080"
    volumes:
      - /opt/civilization/openwebui:/app/backend/data
      - /opt/civilization/library/pdfs:/data/pdfs:ro 
    environment:
      # Point to the HOST machine's Ollama
      - OLLAMA_BASE_URL=http://host.docker.internal:11434
    networks:
      - civilization_net
    extra_hosts:
      - "host.docker.internal:172.17.0.1"

  kiwix:
    image: ghcr.io/kiwix/kiwix-serve:latest
    container_name: civ_library
    restart: unless-stopped
    ports:
      - "8080:8080"
    volumes:
      - /opt/civilization/library/zims:/data
    command: "*.zim"
    networks:
      - civilization_net
    healthcheck:
      test: ["CMD-SHELL", "wget --spider -q http://localhost:8080 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3

networks:
  civilization_net:
    driver: bridge
